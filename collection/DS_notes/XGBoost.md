---
layout: project
title: 'XGBoost'
date: 01 July 2020

---		
## Overview
- XGBoost是GBDT算法的优化

## 核心
- 串行算法：下一颗树的生长方式依赖于上一棵树
- 加法模型：多个树模型加权结合
- 基学习器
  - 基学习器是一系列回归树CART
  - 每棵树的训练目标：最小化目标函数【包括损失函数和正则化项】
  - 显式正则化：想比原始GBDT，XGBoost的目标函数多了正则化项（L1/L2正则化，对树的复杂度进行惩罚）->使得学习出来的模型更加不容易过拟合。
  - 行采样 + 列采样：类似于随机森林，XGBoost在构建树的过程中，加入了样本采样和特征采样，防止过拟合
  - 缺失值：加入了对缺失值的处理，xgboost分别将缺失样本放置到左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为该样本的分裂方向
  - 特征并行化：XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，提高训练效率
- 把目标函数转化成树结构的叶子得分：将函数映射到叶子节点上,由样本损失的叠加【GBDT】，转换成叶子节点的得分
- 解决优化问题：使用牛顿法进行优化，对目标函数进行二阶泰勒展开【同时用到一阶导和二阶导】->可以更为精准的逼近真实的损失函数
  - 函数更新的方向是 - gradient/ hessian matrix

## 单个树如何分裂
- 贪心法：每次尝试分裂一个叶节点，遍历所有的分裂可能性，计算分裂前后的增益，选择增益最大的
- Level-wise split：广度优先算法，对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，
  - 加重了计算代价

## 多个树如何集成
- 每棵树输出的是叶子节点的得分【leaf score】，对所有树的得分进行求和
- 如何使用得分总和跟loss function有关系
  - 回归树可以直接作为目标值使用 ->每棵树的得分在不断修正最终结果
  - 分类树要代入sigmoid function， 映射为概率
- 每增加一棵树，影响的不是最终结果；而是每增加一棵树，影响得分，最终影响sigmoid函数输出的概率

## 如何判断特征重要性
- weight:特征用来作为切分特征的次数
- gain:使用特征进行切分的平均增益
- cover:各个树中该特征平均覆盖情况，通过节点的二阶梯度和来度量

## 重要参数
- Objective目标函数种类
  - reg: linear
  - binary: logistic
  - multi: softmax
- eval_metric
  - 回归： rmse/mae
  - 分类：logloss/error/merror/mlogloss/auc
- Boosting参数
  - n_estimator: 生成树的数目，也是最大迭代次数
  - learning_rate / eta: 学习率，默认0.3. 学习率太大找不到拟合点，太小运行速度满，一般0.1左右
- 树参数
  - gamma：节点分裂时损失函数的下降值，小于此值不分裂。Gamma越大模型越保守 
  - subsample：每棵树的样本采样比例，一般(0.5, 1],太小会欠拟合
  - colsample_bytree: 每棵树的特征采样比例，一般0.8左右， 相应的还有colsample_bylevel, colsample_bynode
  - max_depth: 最大深度
  - min_child_weight：叶子节点最小样本数，小于此值停止分裂
  - Alpha: L1正则化
  - lambda：L2正则化
  - scale_pos_weight:正负样本比值。负样本总数/正样本总数
