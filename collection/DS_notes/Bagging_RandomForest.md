---
layout: project
title: 'Bagging & Random Forest'
date: 01 July 2020

---
## Bootstrapping
- Bootstrapping思想：一种统计抽样方法，通过有放回的方法从总样本中选取部分样本，允许重复

## Bagging
- bagging是boostrapping思想在机器学习领域的应用，是bootstrap aggregating的缩写
- Bagging核心：
  - 有放回的方式选取训练集
  - 利用样本扰动建立差异化的模型，对结果进行平均或投票 -> 有助于降低方差
- Bagging步骤
  - 有m个样本的训练集
  - 对训练集进行T次有放回的抽样，每次抽出m个样本-> T个采样集
	- 建立T个学习器，对预估结果进行平均或投票
- Bagging的优势
  - 改善过拟合，降低模型方差 
  - 并行式的集成学习方法：学习器的训练之间没有前后顺序可以同时进行
  - 适合能力强但容易表达过度的模型：决策树和神经网络

## 随机森林
- Random Forest是一种决策树Bagging模型的优化版本
- 核心思想依然是bagging
- 森林【必须使用CART决策树作为基学习器，多棵树组成森林】 + 随机【Bagging + 特征随机】
- 步骤
  - 样本抽样【有放回的选择样本，Bagging模式】
    - 对m个样本训练集进行T次有放回的抽样，每次抽取m个样本 ->T个采样集
    - 构建T个CART决策树
  - 特征抽样
    - 在每个树的节点处，先抽取k个特征子集
    - 再从子集中选取最优特征进行生长
   - 每棵树最大限度地生长，尽量overfitting,不做剪枝
   - 多棵树组成随机森林，对预估结果做平均或投票
- 本质：双重随机性/扰动 -> 利用树的差异性控制过拟合，提高泛化能力
 
## 随机森林 vs. Bagged Trees
- 随机森林的起始性能较差（单棵树的准确度有所下降）
- 随着树的数量增多，往往会收敛到比较低的泛化误差
- 只在特征子集选取最优划分属性，训练效率更高

## 随机森林重要参数
- Bagging参数
  - n_estimators: 决策树的个数，默认100。n_estimators太小容易欠拟合，n_estimators太大训练耗时。随着树的数目增加，模型的表现会提升，但是到了一定程度，提升会放缓。通常选择50~200之间的数字。
  - oob_score :即是否采用袋外样本来评估模型的好坏。默认识False。
  - criterion: CART树选取最优特征的指标。分类模型默认是gini index,回归树默认是mse。
- 树参数
  - max_features: 树节点处的最大特征数，这是体现树之间差异性从而提升泛化能力的重要参数。默认”auto”是sqrt(N)个特征。可选“log2”即log2N个特征;整数代表考虑的特征绝对数;浮点数代表特征百分比。其中N为样本总特征数。可以通过调参尝试不同参数值。
  - max_depth：树的的最大深度，默认不限制深度。如果样本量多，特征多的时候，推荐限制这个最大深度，常用取值10-100之间。
  - min_sample_split:限制了子树继续划分的条件,默认是2。如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 如果样本量不大，不需要管这个值。如果样本量数量级非常大，推荐增大这个值。
  - min_samples_leaf: 叶子节点最少的样本树，默认1。如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。对于随机森林来说，树越大越好，不需要管这个值。如果样本量非常大，在意训练速度，可以增大这个值，当模型表现变坏的之前停止。
