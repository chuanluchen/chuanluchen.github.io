---
layout: project
title: '线性回归'
date: 01 July 2020

---
## Overview
- 线性回归是利用特征的线性组合去拟合空间中点的分布和轨迹，进而对连续值结果进行预测
- 优：简单，基本，可解释性好
- 劣：只适合表达线性关系
- 劣：特征工程量大

## 目标
- 找到一条直线最好地去接近【拟合】所有样本点

<br>
<img src="/assets/img/knowledge/linear_regression/linear_regression.jpg"  width='80%'/>
<br><br>

## 参数
- 解读系数θ的涵义：在其它变量保持不变的情况下，x增加1个单位对y产生的平均效果

## 损失函数是凸函数，求解参数的两种方法
### 最小二乘法
- 令损失函数导数= 0，求解参数
- 劣：当数据量大，求矩阵的逆耗时 ； 可能矩阵不可逆

### 方法2：使用梯度下降法找到全局最低点
$$
\boldsymbol{\theta}^{t}=\boldsymbol{\theta}^{t-1}-\alpha L^{\prime}\left(\theta^{t-1}\right)
$$
- 梯度下降：损失函数针对参数迭代求导 -> 得到梯度方向，沿着负梯度的方向做迭代，直到最低点
- 学习率问题：迭代过程中学习率决定迭代步子的大小：α过大->找不到最低点，甚至无法收敛； α太小，收敛太慢

## 模型优化方向
- 加入交互项 Interaction
- 加入非线性：多项式回归
- 正则化 regularization






