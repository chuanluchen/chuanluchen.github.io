---
layout: project
title: 'PCA主成分分析'
date: 01 July 2020

---
## Overview
- 通过线性变换，把数据从高维压缩到低维空间，同时保留其中主要的信息 
- 找到一个尽可能包含足够多数据及变异信息的低维表示

## 核心
- PCA的思想是将n维特征映射到k维上（k < n）
- 这k维称为主成分，是数据在主方向上的投影，是全新构造出来的k维正交特征，而不是简单地从n维特征中去除其余n-k维特征
- 主成分代表了原始数据高度变异的方向，第一主成分方差最大，依次递减

## 原理
- 关键：整个PCA过程就是求协方差的特征值和特征向量，然后做数据转换。
- 矩阵相乘可以实现空间转换：基【用于空间转换的基准矩阵】* 原始矩阵 = 原始矩阵在基的空间内的投影 -> 若基的维度低，可实现降维
- 最大方差理论【N维特征中找最好的K维特征，保留主要信息】: 将n维样本点转换为k维后，每一维上的样本方差都尽可能大， 每个维度之间相关度低（正交，协方差为0) 
- 如何找到空间转换的基：协方差矩阵的特征向量
  - 降维的目标：使协方差矩阵对角化【非对角线元素为0，对角元素按从大到小依次排列】
  - 推导公式得到：转换后的协方差矩阵D = P * 转换前的协方差C * P<sup>T</sup>  ->目标是寻找从成这种转换的矩阵P
  - 根据特征值/特征向量定义：P即是协方差矩阵的特征向量构成的矩阵
	- 第一主成分是特征值最大时对应的特征向量，其次是特征值第二大对应的特征向量，依次类推。

## 实现步骤
- 假设有m条n维数据
1. 每行放一个特征：将原始数据按列组成n行m列矩阵X
2. 零均值化：将X的每一行(一行一个字段)进行零均值化（每个值-行均值）-> 减去均值后的样本矩阵为DataAdjust(n * m)
3. 求出协方差矩阵C= 1/m XX<sup>T</sup> 【对角线上是方差，非对角线上是协方差】
4. 求出协方差矩阵的特征值及对应的特征向量
5. 将特征向量按对应特征值大小从上到下排列成矩阵，取前K行组成特征向量矩阵P【EigenVectors(k * n)】 
6. 将样本点投影到选取的特征向量上【特征向量矩阵 X样本矩阵 】, Y=PX即为降维到K维后的数据
		FinalData(k * m)  = EigenVectors(k * n) X DataAdjust(n * m)

## 优劣
- 优：很好解决线性相关
- 劣：对非线性不理想
